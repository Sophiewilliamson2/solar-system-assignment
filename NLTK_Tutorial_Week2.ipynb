{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sophiewilliamson2/solar-system-assignment/blob/main/NLTK_Tutorial_Week2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEkA2hP6vVlk"
      },
      "source": [
        "# Natural Language Processing With Python's NLTK (Week 2)\n",
        "_Starter notebook generated for Sophie (MSc AI)._\n",
        "\n",
        "**Topics:** Getting Started · Tokenizing · Stop Words · Stemming · POS Tagging · Lemmatizing · Chunking · Chinking · NER · Concordance · Dispersion Plot · Frequency Distribution · Collocations\n",
        "\n",
        "> Each section mirrors the Real Python tutorial."
      ],
      "id": "WEkA2hP6vVlk"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIcpcmxLvVlm"
      },
      "source": [
        "# DISCLAIMER: This code was assisted by ChatGPT.\n",
        "# --- Setup: installs & NLTK data ---\n",
        "!pip -q install nltk==3.8.1 matplotlib numpy\n",
        "import nltk\n",
        "nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger'); nltk.download('maxent_ne_chunker'); nltk.download('words')\n",
        "nltk.download('book')\n",
        "print('Setup complete.')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "bIcpcmxLvVlm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWZl11qjvVlm"
      },
      "source": [
        "## 1) Tokenizing (sentences & words)"
      ],
      "id": "EWZl11qjvVlm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwMDXJnYvVlm"
      },
      "source": [
        "# DISCLAIMER: This code was assisted by ChatGPT.\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "example_string = (\n",
        "    \"\"\"\n",
        "Muad'Dib learned rapidly because his first training was in how to learn.\n",
        "And the first lesson of all was the basic trust that he could learn.\n",
        "It's shocking to find how many people do not believe they can learn,\n",
        "and how many more believe learning to be difficult.\n",
        "\"\"\"\n",
        ")\n",
        "sents = sent_tokenize(example_string)\n",
        "words = word_tokenize(example_string)\n",
        "print('Sentences:', sents)\n",
        "print('\\nFirst 20 word tokens:', words[:20])"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "gwMDXJnYvVlm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ondm1fFFvVln"
      },
      "source": [
        "## 2) Filtering Stop Words"
      ],
      "id": "ondm1fFFvVln"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHmn02x4vVln"
      },
      "source": [
        "# DISCLAIMER: This code was assisted by ChatGPT.\n",
        "from nltk.corpus import stopwords\n",
        "worf_quote = \"Sir, I protest. I am not a merry man!\"\n",
        "tokens = word_tokenize(worf_quote)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered = [w for w in tokens if w.casefold() not in stop_words]\n",
        "print('Original:', tokens)\n",
        "print('Filtered:', filtered)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "lHmn02x4vVln"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3JdBzwTvVln"
      },
      "source": [
        "## 3) Stemming (Porter)"
      ],
      "id": "t3JdBzwTvVln"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jRe5p6jvVln"
      },
      "source": [
        "# DISCLAIMER: This code was assisted by ChatGPT.\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "string_for_stemming = (\n",
        "    \"\"\"\n",
        "The crew of the USS Discovery discovered many discoveries.\n",
        "Discovering is what explorers do.\n",
        "\"\"\"\n",
        ")\n",
        "tokens = word_tokenize(string_for_stemming)\n",
        "stems = [stemmer.stem(w) for w in tokens]\n",
        "list(zip(tokens, stems))[:20]"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "3jRe5p6jvVln"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7JLrap2vVln"
      },
      "source": [
        "## 4) POS Tagging"
      ],
      "id": "R7JLrap2vVln"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae2_TCxAvVln"
      },
      "source": [
        "# DISCLAIMER: This code was assisted by ChatGPT.\n",
        "sagan_quote = (\n",
        "    \"\"\"\n",
        "If you wish to make an apple pie from scratch,\n",
        "you must first invent the universe.\n",
        "\"\"\"\n",
        ")\n",
        "words_in_sagan = word_tokenize(sagan_quote)\n",
        "import nltk\n",
        "pos_tags = nltk.pos_tag(words_in_sagan)\n",
        "pos_tags[:20]"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "ae2_TCxAvVln"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-wysLOqvVln"
      },
      "source": [
        "## 5) Lemmatizing (WordNet)"
      ],
      "id": "A-wysLOqvVln"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny9SvhIpvVln"
      },
      "source": [
        "# DISCLAIMER: This code was assisted by ChatGPT.\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print('scarves ->', lemmatizer.lemmatize('scarves'))\n",
        "print('worst (noun) ->', lemmatizer.lemmatize('worst'))\n",
        "print('worst (adjective) ->', lemmatizer.lemmatize('worst', pos='a'))\n",
        "string_for_lemmatizing = 'The friends of DeSoto love scarves.'\n",
        "words2 = word_tokenize(string_for_lemmatizing)\n",
        "lemmas = [lemmatizer.lemmatize(w) for w in words2]\n",
        "list(zip(words2, lemmas))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "Ny9SvhIpvVln"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7SQoSQ1vVlo"
      },
      "source": [
        "## 6) Chunking (NP grammar)"
      ],
      "id": "p7SQoSQ1vVlo"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wgio_NlHvVlo"
      },
      "source": [
        "# DISCLAIMER: This code was assisted by ChatGPT.\n",
        "lotr_quote = \"It's a dangerous business, Frodo, going out your door.\"\n",
        "words_in_lotr = word_tokenize(lotr_quote)\n",
        "lotr_pos = nltk.pos_tag(words_in_lotr)\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "chunk_parser = nltk.RegexpParser(grammar)\n",
        "tree = chunk_parser.parse(lotr_pos)\n",
        "tree"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "Wgio_NlHvVlo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-KFBtTzvVlo"
      },
      "source": [
        "## 7) Chinking (exclude adjectives)"
      ],
      "id": "L-KFBtTzvVlo"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtjCI7NmvVlo"
      },
      "source": [
        "# DISCLAIMER: This code was assisted by ChatGPT.\n",
        "grammar2 = \"\"\"\n",
        "Chunk: {<.*>+}\n",
        "       }<JJ>{\n",
        "\"\"\"\n",
        "chunk_parser2 = nltk.RegexpParser(grammar2)\n",
        "tree2 = chunk_parser2.parse(lotr_pos)\n",
        "tree2"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "OtjCI7NmvVlo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwZ0Hz46vVlo"
      },
      "source": [
        "## 8) Named Entity Recognition (NER)"
      ],
      "id": "fwZ0Hz46vVlo"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZIQiUmmvVlo"
      },
      "source": [
        "# DISCLAIMER: This code was assisted by ChatGPT.\n",
        "ner_tree = nltk.ne_chunk(lotr_pos)\n",
        "ner_tree"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "MZIQiUmmvVlo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GGUltNkvVlo"
      },
      "source": [
        "## 9) NLTK Book Corpora"
      ],
      "id": "5GGUltNkvVlo"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8cZGpTlvVlo"
      },
      "source": [
        "# DISCLAIMER: This code was assisted by ChatGPT.\n",
        "from nltk.book import *  # noqa\n",
        "texts()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "d8cZGpTlvVlo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcV-gAyDvVlo"
      },
      "source": [
        "## 10) Concordance"
      ],
      "id": "zcV-gAyDvVlo"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqCZLY_tvVlo"
      },
      "source": [
        "# DISCLAIMER: This code was assisted by ChatGPT.\n",
        "text8.concordance('man')\n",
        "text8.concordance('woman')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "AqCZLY_tvVlo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzFCabONvVlo"
      },
      "source": [
        "## 11) Dispersion Plot"
      ],
      "id": "pzFCabONvVlo"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCDvEhJevVlo"
      },
      "source": [
        "# DISCLAIMER: This code was assisted by ChatGPT.\n",
        "text8.dispersion_plot(['woman','lady','girl','gal','man','gentleman','boy','guy'])\n",
        "text2.dispersion_plot(['Allenham','Whitwell','Cleveland','Combe'])"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "GCDvEhJevVlo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLsB5OQVvVlo"
      },
      "source": [
        "## 12) Frequency Distribution"
      ],
      "id": "lLsB5OQVvVlo"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FNnZtGVvVlo"
      },
      "source": [
        "# DISCLAIMER: This code was assisted by ChatGPT.\n",
        "from nltk import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "fd_all = FreqDist(text8)\n",
        "print(fd_all.most_common(20))\n",
        "stop = set(stopwords.words('english'))\n",
        "meaningful = [w for w in text8 if w.casefold() not in stop]\n",
        "fd_meaningful = FreqDist(meaningful)\n",
        "print(fd_meaningful.most_common(20))\n",
        "fd_meaningful.plot(20, cumulative=True)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "4FNnZtGVvVlo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAk4tPvJvVlo"
      },
      "source": [
        "## 13) Collocations (raw vs lemmatized)"
      ],
      "id": "dAk4tPvJvVlo"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2ry5ePdvVlo"
      },
      "source": [
        "# DISCLAIMER: This code was assisted by ChatGPT.\n",
        "print('Raw collocations:')\n",
        "text8.collocations()\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = [lemmatizer.lemmatize(w) for w in text8]\n",
        "new_text = nltk.Text(lemmatized)\n",
        "print('\\nLemmatized collocations:')\n",
        "new_text.collocations()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "e2ry5ePdvVlo"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}